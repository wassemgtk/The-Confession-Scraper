The other day when I got that crime alert email from the university I confess that there was something that really bothered me. In the "precautions" guidelines at the bottom there is a line that says "if you are confronted by a suspect, give them what they want and don't chase them as they leave."

Obviously if someone has a weapon on you and demands money the safest thing to do is comply. And obviously the money in your wallet is not worth dying for. I'm not arguing that. And I'm not blaming the Wash U administration for trying to keep us safe. But to me it just seems like such a sad thing when good, innocent people are told to just do what a criminal says simply because it's easier.

Maybe I'm reading too much into this one line and maybe it's nothing, but to me it seems to highlight a trend in our society today. I wish more people stressed the importance of self-defense (whether it's through pepper spray or karate lessons or even firearms, whatever), especially for women (I'm a girl.) A lot of times we hear "well, you should teach your sons not to rape and murder and steal", and I'm all for that. Two thumbs up. But in the meantime, why don't we ever hear "teach your daughters how to defend themselves"? It makes me so angry that I should have to be scared of people who would want do me harm. I'm seriously considering taking self-defense lessons.